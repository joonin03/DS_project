from huggingface_hub import login
login()

import os
import logging
import torch
import numpy as np
import pdfplumber
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer, CrossEncoder
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from rank_bm25 import BM25Okapi
import chromadb
from chromadb.config import Settings

# ==========================================
# 1. ì„¤ì • ë° ë¡œê¹… (Configuration & Logging)
# ==========================================

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger("MedicalRAG")

@dataclass
class AppConfig:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ ì„¤ì • ê´€ë¦¬"""
    # ëª¨ë¸ ê²½ë¡œ
    llm_model_id: str = "google/txgemma-9b-chat"
    embedding_model_id: str = "jhgan/ko-sroberta-multitask"
    reranker_model_id: str = "BAAI/bge-reranker-v2-m3"

    # ë²¡í„° DB ì„¤ì •
    collection_name: str = "medical_kb"
    chroma_path: str = "./chroma_db"  # ì˜êµ¬ ì €ì¥ ê²½ë¡œ (ì„ íƒ ì‚¬í•­)

    # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
    initial_retrieval_k: int = 10  # 1ì°¨ ê²€ìƒ‰ ê°œìˆ˜
    final_top_k: int = 3           # ìµœì¢… Reranking í›„ ê°œìˆ˜
    rrf_k_constant: int = 60       # RRF ìƒìˆ˜

    # ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •
    chunk_size: int = 800
    chunk_overlap: int = 350

    # ìƒì„± ì„¤ì •
    max_new_tokens: int = 768
    temperature: float = 0.1
    top_p: float = 0.9

config = AppConfig()

# ==========================================
# 2. ë°ì´í„° êµ¬ì¡° (Data Structures)
# ==========================================

@dataclass
class SearchResult:
    """ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ êµ¬ì¡°ì²´"""
    text: str
    source: str
    score: float = 0.0

@dataclass
class RAGResponse:
    """ìµœì¢… ë‹µë³€ êµ¬ì¡°ì²´"""
    query: str
    answer: str
    sources: List[str]
    processing_time: float = 0.0

# ==========================================
# 3. ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“ˆ (Document Processor)
# ==========================================

class DocumentProcessor:
    """PDF ë¡œë”© ë° ì²­í‚¹ ë‹´ë‹¹"""

    @staticmethod
    def _table_to_markdown(table: List[List[str]]) -> str:
        """PDF í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not table or len(table) < 2:
            return ""
        try:
            # None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ê³  ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±°
            table = [['' if cell is None else str(cell).replace('\n', ' ') for cell in row] for row in table]

            markdown = "| " + " | ".join(table[0]) + " |\n"
            markdown += "| " + " | ".join(["---"] * len(table[0])) + " |\n"
            for row in table[1:]:
                markdown += "| " + " | ".join(row) + " |\n"
            return markdown + "\n"
        except Exception as e:
            logger.warning(f"í‘œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    @classmethod
    def process_pdf(cls, file_path: str) -> List[Dict[str, str]]:
        """PDF íŒŒì¼ì„ ì½ì–´ ì²˜ë¦¬ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {file_path}")
        docs = []

        try:
            with pdfplumber.open(file_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text() or ""
                    tables = page.extract_tables()
                    table_text = "".join([cls._table_to_markdown(t) for t in tables if t])

                    # í‘œ ë‚´ìš©ê³¼ í…ìŠ¤íŠ¸ ê²°í•©
                    combined_content = f"{text}\n\n[í‘œ ë°ì´í„°]\n{table_text}"
                    docs.append(Document(
                        page_content=combined_content,
                        metadata={"source": f"{os.path.basename(file_path)} (p.{i + 1})"}
                    ))
        except Exception as e:
            logger.error(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")
            raise e

        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            separators=["\n\n", "\n", "|", ".", " "]
        )
        splits = text_splitter.split_documents(docs)

        logger.info(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(splits)} ì²­í¬")
        return [{"text": doc.page_content, "source": doc.metadata['source']} for doc in splits]

# ==========================================
# 4. ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ (Retrieval Engine)
# ==========================================

class HybridRetriever:
    """Vector DB + BM25 + Reranker ê²€ìƒ‰ ì—”ì§„"""

    def __init__(self, cfg: AppConfig, tokenizer):
        self.cfg = cfg
        self.tokenizer = tokenizer # BM25 í† í°í™”ìš©

        # ëª¨ë¸ ë¡œë“œ
        logger.info("SEARCH: ì„ë² ë”© ë° ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embed_model = SentenceTransformer(cfg.embedding_model_id)
        self.reranker = CrossEncoder(
            cfg.reranker_model_id,
            automodel_args={"torch_dtype": torch.float16},
            trust_remote_code=True
        )

        # ë²¡í„° DB ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ëª¨ë“œ)
        self.chroma_client = chromadb.Client(Settings(allow_reset=True))
        self.chroma_client.reset() # ì´ˆê¸°í™”
        self.collection = self.chroma_client.create_collection(cfg.collection_name)

        # Sparse ê²€ìƒ‰(BM25)ìš© ìƒíƒœ
        self.bm25 = None
        self.documents_map = {} # ID -> Document ë§¤í•‘

    def index_documents(self, documents: List[Dict[str, str]]):
        """ë¬¸ì„œ ì¸ë±ì‹± (Vector DB + BM25)"""
        if not documents:
            logger.warning("ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"SEARCH: {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...")
        texts = [doc['text'] for doc in documents]
        ids = [f"doc_{i}" for i in range(len(texts))]

        # 1. BM25 ì¸ë±ì‹±
        tokenized_corpus = [self.tokenizer.tokenize(doc) for doc in texts]
        self.bm25 = BM25Okapi(tokenized_corpus)

        # 2. ë¬¸ì„œ ë§µ ì €ì¥
        for i, doc_id in enumerate(ids):
            self.documents_map[doc_id] = {
                "text": texts[i],
                "source": documents[i]['source']
            }

        # 3. ë²¡í„° DB ì €ì¥
        embeddings = self.embed_model.encode(texts, convert_to_numpy=True)
        self.collection.add(
            documents=texts,
            embeddings=embeddings.tolist(),
            ids=ids
        )
        logger.info("âœ… ì¸ë±ì‹± ì™„ë£Œ.")

    def search(self, query: str) -> List[SearchResult]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­í‚¹ ì‹¤í–‰"""
        if not self.bm25 or not self.documents_map:
            logger.warning("ì¸ë±ì‹±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        k = self.cfg.initial_retrieval_k

        # 1. Dense Retrieval (ë²¡í„°)
        query_vec = self.embed_model.encode(query).tolist()
        vec_res = self.collection.query(query_embeddings=[query_vec], n_results=k)
        vec_ids = vec_res['ids'][0] if vec_res['ids'] else []

        # 2. Sparse Retrieval (BM25)
        tokenized_query = self.tokenizer.tokenize(query)
        bm25_scores = self.bm25.get_scores(tokenized_query)
        top_n_indices = np.argsort(bm25_scores)[::-1][:k]
        bm25_ids = [f"doc_{i}" for i in top_n_indices]

        # 3. RRF Fusion (ìˆœìœ„ ì¬ì¡°ì •)
        rrf_score = {}
        for rank, doc_id in enumerate(vec_ids):
            rrf_score[doc_id] = rrf_score.get(doc_id, 0) + 1 / (rank + self.cfg.rrf_k_constant)
        for rank, doc_id in enumerate(bm25_ids):
            rrf_score[doc_id] = rrf_score.get(doc_id, 0) + 1 / (rank + self.cfg.rrf_k_constant)

        # ìƒìœ„ í›„ë³´ ì¶”ì¶œ
        sorted_candidates = sorted(rrf_score.items(), key=lambda item: item[1], reverse=True)
        candidate_ids = [doc_id for doc_id, _ in sorted_candidates[:k]]

        # 4. Reranking (ì •ë°€ ê²€ì¦)
        candidate_texts = [self.documents_map[doc_id]['text'] for doc_id in candidate_ids]
        pairs = [[query, text] for text in candidate_texts]
        rerank_scores = self.reranker.predict(pairs)

        # ìµœì¢… ì •ë ¬ ë° ê²°ê³¼ ë°˜í™˜
        final_results = []
        for doc_id, score in sorted(zip(candidate_ids, rerank_scores), key=lambda x: x[1], reverse=True)[:self.cfg.final_top_k]:
            doc_info = self.documents_map[doc_id]
            final_results.append(SearchResult(
                text=doc_info['text'],
                source=doc_info['source'],
                score=float(score)
            ))

        return final_results

# ==========================================
# 5. ìƒì„± ì—”ì§„ ëª¨ë“ˆ (Generation Engine)
# ==========================================

class LLMGenerator:
    """LLM ë¡œë“œ ë° ë‹µë³€ ìƒì„± ë‹´ë‹¹"""

    PROMPT_TEMPLATE = """<start_of_turn>user
ë‹¹ì‹ ì€ 'ê·¼ê±° ì¤‘ì‹¬ ì˜í•™(Evidence-Based Medicine)'ì„ ì¤€ìˆ˜í•˜ëŠ” ì˜ë£Œ AI ì „ë¬¸ì˜ì…ë‹ˆë‹¤.
ì•„ë˜ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ê²€ìƒ‰ëœ ë¬¸ì„œ]
{context}

[ë‹µë³€ ì‘ì„± ì›ì¹™]
1. **ë‹¨ê³„ë³„ ì¶”ë¡ **: ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ íŒ©íŠ¸ë¥¼ ì°¾ì€ ë’¤ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.
2. **ì—„ê²©í•œ êµ¬ë¶„**: ë¬¸ì„œ ë‚´ì˜ 'ê¸ˆê¸°ì‚¬í•­(Contraindications)', 'ì£¼ì˜ì‚¬í•­(Warnings)', 'ê¶Œê³ (Indications/Target)'ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”.
3. **ì˜¤í•´ ê¸ˆì§€**: í‘œ ì‘ì„± ì‹œ 'ìœ„í—˜(Risk)'ì´ë¼ëŠ” ë‹¨ì–´ê°€ ì˜¤í•´ë¥¼ ì‚¬ì§€ ì•Šë„ë¡ 'ì„ìƒì  ì˜í–¥' ë“±ìœ¼ë¡œ ëª…í™•íˆ í•˜ì„¸ìš”.
4. **ì‚¬ì‹¤ ê¸°ë°˜**: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
5. **ì¶œì²˜ ì¸ìš©**: ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš©ì´ ë¬¸ì„œ ì–´ë””ì— ìˆëŠ”ì§€ ì°¸ê³ í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}<end_of_turn>
<start_of_turn>model
[ë¶„ì„ ë‹¨ê³„]
1. ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ì˜ë„ íŒŒì•…: """

    def __init__(self, cfg: AppConfig):
        self.cfg = cfg
        logger.info(f"GEN: LLM ëª¨ë¸ ë¡œë”© ì¤‘ ({cfg.llm_model_id})...")

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )

        self.tokenizer = AutoTokenizer.from_pretrained(cfg.llm_model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            cfg.llm_model_id,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        logger.info("âœ… LLM ë¡œë”© ì™„ë£Œ.")

    def generate(self, query: str, context_docs: List[SearchResult]) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ë‹µë³€ ìƒì„±"""
        # ë¬¸ë§¥ ì¡°í•©
        context_str = "\n\n".join([f"ë¬¸ì„œ[{i+1}]: {doc.text}" for i, doc in enumerate(context_docs)])

        prompt = self.PROMPT_TEMPLATE.format(context=context_str, query=query)

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        try:
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.cfg.max_new_tokens,
                    temperature=self.cfg.temperature,
                    top_p=self.cfg.top_p,
                    repetition_penalty=1.1,
                    do_sample=True
                )

            full_response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            return full_response.strip()

        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ==========================================
# 6. ë©”ì¸ ì‹œìŠ¤í…œ (Orchestrator)
# ==========================================

class MedicalRAGSystem:
    """ì „ì²´ RAG ì‹œìŠ¤í…œì„ ê´€ì¥í•˜ëŠ” Facade í´ë˜ìŠ¤"""

    def __init__(self, config: AppConfig = AppConfig()):
        self.cfg = config

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.generator = LLMGenerator(config)
        self.retriever = HybridRetriever(config, self.generator.tokenizer)

    def ingest_file(self, file_path: str):
        """íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
        try:
            processed_docs = DocumentProcessor.process_pdf(file_path)
            self.retriever.index_documents(processed_docs)
            return True
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False

    def ask(self, query: str) -> RAGResponse:
        """ì§ˆë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        logger.info(f"â“ ì§ˆë¬¸ ìˆ˜ì‹ : {query}")

        # 1. ê²€ìƒ‰
        search_results = self.retriever.search(query)

        if not search_results:
            return RAGResponse(query=query, answer="ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", sources=[])

        # 2. ìƒì„±
        answer = self.generator.generate(query, search_results)

        # 3. ì¶œì²˜ ì •ë¦¬
        sources = list(set([doc.source for doc in search_results]))

        return RAGResponse(query=query, answer=answer, sources=sources)

# ==========================================
# 7. ì‹¤í–‰ ì˜ˆì œ (Usage)
# ==========================================

if __name__ == "__main__":
    import time

    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = MedicalRAGSystem()

    # 2. ë°ì´í„° ì£¼ì… (ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”)
    # ì—…ë¡œë“œí•˜ì‹  íŒŒì¼ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì‹œ ê²½ë¡œ ì„¤ì •
    pdf_path = "/content/drive/MyDrive/[ëŒ€í•œì˜í•™íšŒ] ë‹¹ë‡¨ë³‘ ì„ìƒì§„ë£Œì§€ì¹¨.pdf"

    if os.path.exists(pdf_path):
        rag_system.ingest_file(pdf_path)

        # 3. ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        questions = [
            "ë‹¹ë‡¨ë³‘ í™˜ìì˜ ìš´ë™ ë¶€í•˜ ê²€ì‚¬ ê¸ˆê¸° ì‚¬í•­ì€ ë­ì•¼?",
            "SGLT2 ì–µì œì œì™€ DPP-4 ì–µì œì œì˜ ì²´ì¤‘ ì˜í–¥ ì°¨ì´ë¥¼ í‘œë¡œ ë¹„êµí•´ì¤˜."
        ]

        for q in questions:
            print(f"\n[ì§ˆë¬¸] {q}")
            start_t = time.time()
            response = rag_system.ask(q)
            end_t = time.time()

            print(f"[ë‹µë³€]\n{response.answer}")
            print(f"[ì¶œì²˜] {response.sources}")
            print(f"[ì†Œìš”ì‹œê°„] {end_t - start_t:.2f}ì´ˆ")
            print("-" * 50)
    else:
        logger.warning(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {pdf_path}")