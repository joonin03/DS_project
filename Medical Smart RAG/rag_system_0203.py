# -*- coding: utf-8 -*-
"""rag_system_0203

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5OAw6t8lzXfyEnJk2Nuwq4IcP-ts7dy
"""

!pip install -U transformers langchain langchain-community langchain-huggingface datasets
!pip install sentence-transformers chromadb accelerate bitsandbytes pdfplumber rank_bm25 gradio

from huggingface_hub import login
login("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")

# -*- coding: utf-8 -*-
import os
import time
import logging
import torch
import numpy as np
import pdfplumber
import gradio as gr
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from google.colab import userdata
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer, CrossEncoder
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from rank_bm25 import BM25Okapi
import chromadb
from chromadb.config import Settings

# ==========================================
# 0. í™˜ê²½ ì„¤ì • ë° ì¸ì¦ (Setup & Auth)
# ==========================================

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger("MedicalRAG")



@dataclass
class AppConfig:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • (Configuration)"""
    # Model config
    llm_model_id: str = "google/txgemma-27b-chat"
    embedding_model_id: str = "jhgan/ko-sroberta-multitask"
    reranker_model_id: str = "BAAI/bge-reranker-v2-m3"

    # Vector DB config
    collection_name: str = "medical_kb"

    # RAG Parameters
    chunk_size: int = 800
    chunk_overlap: int = 350
    initial_retrieval_k: int = 10
    final_top_k: int = 3

    # Gen Parameters
    max_new_tokens: int = 768
    temperature: float = 0.1

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ DTO"""
    text: str
    source: str
    score: float = 0.0

@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ DTO"""
    query: str
    answer: str
    sources: List[str]

# ==========================================
# 1. ë¬¸ì„œ ì²˜ë¦¬ (Document Processing)
# ==========================================
class DocumentProcessor:
    """PDF íŒŒì‹± ë° ì²­í‚¹ ì „ëµ íŒ¨í„´"""

    @staticmethod
    def _table_to_markdown(table: List[List[str]]) -> str:
        if not table or len(table) < 2: return ""
        try:
            # None ì²˜ë¦¬ ë° ê°œí–‰ ë¬¸ì ì œê±°
            sanitized_table = [['' if cell is None else str(cell).replace('\n', ' ') for cell in row] for row in table]

            # Markdown ë³€í™˜
            markdown = "| " + " | ".join(sanitized_table[0]) + " |\n"
            markdown += "| " + " | ".join(["---"] * len(sanitized_table[0])) + " |\n"
            for row in sanitized_table[1:]:
                markdown += "| " + " | ".join(row) + " |\n"
            return markdown + "\n"
        except Exception as e:
            logger.warning(f"í‘œ ë³€í™˜ ì˜¤ë¥˜: {e}")
            return ""

    @classmethod
    def process_pdf(cls, file_path: str, config: AppConfig) -> List[Dict[str, str]]:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {file_path}")
        docs = []

        with pdfplumber.open(file_path) as pdf:
            for i, page in enumerate(pdf.pages):
                text = page.extract_text() or ""
                tables = page.extract_tables()
                table_md = "".join([cls._table_to_markdown(t) for t in tables if t])

                content = f"{text}\n\n[í‘œ ë°ì´í„°]\n{table_md}"
                docs.append(Document(
                    page_content=content,
                    metadata={"source": f"{os.path.basename(file_path)} (p.{i+1})"}
                ))

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            separators=["\n\n", "\n", "|", ".", " "]
        )
        splits = text_splitter.split_documents(docs)

        logger.info(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(splits)} ì²­í¬")
        return [{"text": doc.page_content, "source": doc.metadata['source']} for doc in splits]

# ==========================================
# 2. ê²€ìƒ‰ ì—”ì§„ (Retrieval Engine)
# ==========================================
class HybridRetriever:
    """Dense(Vector) + Sparse(BM25) + Reranker ê²€ìƒ‰ ì—”ì§„"""

    def __init__(self, cfg: AppConfig, tokenizer):
        self.cfg = cfg
        self.tokenizer = tokenizer

        logger.info("SEARCH: ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        self.embed_model = SentenceTransformer(cfg.embedding_model_id)
        self.reranker = CrossEncoder(
            cfg.reranker_model_id,
            automodel_args={"torch_dtype": torch.float16},
            trust_remote_code=True
        )

        self.chroma_client = chromadb.Client(Settings(allow_reset=True))
        self.chroma_client.reset()
        self.collection = self.chroma_client.create_collection(cfg.collection_name)

        self.bm25 = None
        self.doc_map = {}

    def index_documents(self, documents: List[Dict[str, str]]):
        if not documents: return

        texts = [d['text'] for d in documents]
        ids = [f"doc_{i}" for i in range(len(texts))]

        # 1. BM25 & Map
        tokenized_corpus = [self.tokenizer.tokenize(t) for t in texts]
        self.bm25 = BM25Okapi(tokenized_corpus)

        for i, doc_id in enumerate(ids):
            self.doc_map[doc_id] = documents[i]

        # 2. Vector DB
        embeddings = self.embed_model.encode(texts, convert_to_numpy=True)
        self.collection.add(documents=texts, embeddings=embeddings.tolist(), ids=ids)
        logger.info(f"SEARCH: {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ.")

    def search(self, query: str) -> List[SearchResult]:
        if not self.bm25: return []

        # 1. Dense Search
        q_vec = self.embed_model.encode(query).tolist()
        vec_res = self.collection.query(query_embeddings=[q_vec], n_results=self.cfg.initial_retrieval_k)
        vec_ids = vec_res['ids'][0] if vec_res['ids'] else []

        # 2. Sparse Search
        bm25_scores = self.bm25.get_scores(self.tokenizer.tokenize(query))
        top_indices = np.argsort(bm25_scores)[::-1][:self.cfg.initial_retrieval_k]
        bm25_ids = [f"doc_{i}" for i in top_indices]

        # 3. RRF Fusion
        rrf = {}
        for rank, doc_id in enumerate(vec_ids):
            rrf[doc_id] = rrf.get(doc_id, 0) + 1 / (rank + 60)
        for rank, doc_id in enumerate(bm25_ids):
            rrf[doc_id] = rrf.get(doc_id, 0) + 1 / (rank + 60)

        candidates = sorted(rrf.items(), key=lambda x: x[1], reverse=True)[:self.cfg.initial_retrieval_k]
        candidate_ids = [c[0] for c in candidates]

        # 4. Reranking
        candidate_texts = [self.doc_map[cid]['text'] for cid in candidate_ids]
        scores = self.reranker.predict([[query, t] for t in candidate_texts])

        results = []
        for cid, score in sorted(zip(candidate_ids, scores), key=lambda x: x[1], reverse=True)[:self.cfg.final_top_k]:
            doc = self.doc_map[cid]
            results.append(SearchResult(text=doc['text'], source=doc['source'], score=float(score)))

        return results

# ==========================================
# 3. ìƒì„± ì—”ì§„ (Generation Engine)
# ==========================================
class LLMGenerator:
    """LLM ë‹µë³€ ìƒì„± ë° í›„ì²˜ë¦¬"""

    PROMPT_TEMPLATE = """<start_of_turn>user
ë‹¹ì‹ ì€ 'ê·¼ê±° ì¤‘ì‹¬ ì˜í•™(Evidence-Based Medicine)'ì„ ì¤€ìˆ˜í•˜ëŠ” ì˜ë£Œ AI ì „ë¬¸ì˜ì…ë‹ˆë‹¤.
ì•„ë˜ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ê²€ìƒ‰ëœ ë¬¸ì„œ]
{context}

[ë‹µë³€ ì‘ì„± ì›ì¹™]
1. **ë‹¨ê³„ë³„ ì¶”ë¡ **: ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ íŒ©íŠ¸ë¥¼ ì°¾ì€ ë’¤ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.
2. **ì—„ê²©í•œ êµ¬ë¶„**: ë¬¸ì„œ ë‚´ì˜ 'ê¸ˆê¸°ì‚¬í•­(Contraindications)', 'ì£¼ì˜ì‚¬í•­(Warnings)', 'ê¶Œê³ (Indications/Target)'ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”.
3. **ì˜¤í•´ ê¸ˆì§€**: í‘œ ì‘ì„± ì‹œ 'ìœ„í—˜(Risk)'ì´ë¼ëŠ” ë‹¨ì–´ê°€ ì˜¤í•´ë¥¼ ì‚¬ì§€ ì•Šë„ë¡ 'ì„ìƒì  ì˜í–¥' ë“±ìœ¼ë¡œ ëª…í™•íˆ í•˜ì„¸ìš”.
4. **ì‚¬ì‹¤ ê¸°ë°˜**: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
5. **ì¶œì²˜ ì¸ìš©**: ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš©ì´ ë¬¸ì„œ ì–´ë””ì— ìˆëŠ”ì§€ ì°¸ê³ í•˜ì„¸ìš”.


ì§ˆë¬¸: {query}<end_of_turn>
<start_of_turn>model """

    def __init__(self, cfg: AppConfig):
        logger.info(f"GEN: LLM ëª¨ë¸ ë¡œë”© ({cfg.llm_model_id})...")
        self.cfg = cfg
        self.tokenizer = AutoTokenizer.from_pretrained(cfg.llm_model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            cfg.llm_model_id,
            quantization_config=BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16
            ),
            device_map="auto"
        )

    def generate(self, query: str, context_docs: List[SearchResult]) -> str:
        context_str = "\n\n".join([f"ë¬¸ì„œ[{i+1}]: {doc.text}" for i, doc in enumerate(context_docs)])
        prompt = self.PROMPT_TEMPLATE.format(context=context_str, query=query)

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.cfg.max_new_tokens,
                temperature=self.cfg.temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                do_sample=True
            )

        full_text = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

        # [Refactoring Point] ë‹µë³€ ì •ì œ ë¡œì§ì„ Generator ë‚´ë¶€ë¡œ ì´ë™ (ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬)
        return self._clean_response(full_text)

    def _clean_response(self, text: str) -> str:
        """CoT(ì‚¬ê³  ê³¼ì •)ë¥¼ ì œê±°í•˜ê³  ìµœì¢… ë‹µë³€ë§Œ ì¶”ì¶œ"""
        if "[ë‹µë³€]" in text:
            return text.split("[ë‹µë³€]")[-1].strip()
        return text.strip()

# ==========================================
# 4. ì‹œìŠ¤í…œ íŒŒì‚¬ë“œ (Facade)
# ==========================================
class MedicalRAGSystem:
    """ì‹œìŠ¤í…œ í†µí•© ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self):
        self.config = AppConfig()
        self.generator = LLMGenerator(self.config)
        self.retriever = HybridRetriever(self.config, self.generator.tokenizer)

    def ingest_file(self, file_path: str) -> bool:
        try:
            docs = DocumentProcessor.process_pdf(file_path, self.config)
            self.retriever.index_documents(docs)
            return True
        except Exception as e:
            logger.error(f"Ingest Error: {e}")
            return False

    def ask(self, query: str) -> RAGResponse:
        results = self.retriever.search(query)
        if not results:
            return RAGResponse(query, "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", [])

        answer = self.generator.generate(query, results)
        sources = list(set([r.source for r in results]))

        return RAGResponse(query, answer, sources)

# ==========================================
# 5. UI í”„ë¡ íŠ¸ì—”ë“œ (Gradio Interface)
# ==========================================
class MedicalWebUI:
    """Gradio UI ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, system: MedicalRAGSystem):
        self.system = system
        self.theme = gr.themes.Soft(
            primary_hue="teal",
            neutral_hue="slate",
            text_size="lg",
            radius_size="md"
        )

    def _adapter_ingest(self, file_obj):
        if file_obj is None: return "âš ï¸ Choose the file."
        if self.system.ingest_file(file_obj.name):
            return f"âœ… Document analysis complete: {os.path.basename(file_obj.name)}"
        return "âŒ Document analysis failure"

    def _adapter_ask(self, message, history):
        if not message: return "", history

        try:
            response = self.system.ask(message)
            answer_text = response.answer

            # ì¶œì²˜ í¬ë§·íŒ…
            if response.sources:
                answer_text += "\n\nğŸ“š Reference Document:\n" + "\n".join([f"- {s}" for s in response.sources])

        except Exception as e:
            answer_text = f"âš ï¸ Error: {str(e)}"

        history.append((message, answer_text))
        return "", history

    def launch(self):
        with gr.Blocks(theme=self.theme, title="Medical AI Assistant") as demo:
            gr.Markdown("# ğŸ©º Medical RAG Chatbot", elem_id="header")
            gr.Markdown("Medical AI Assistant based on MedGemma & RAG")
            gr.Markdown("---")

            with gr.Row():
                # ì¢Œì¸¡ íŒ¨ë„
                with gr.Column(scale=3, min_width=300):
                    gr.Markdown("### ğŸ“‚ Document Analysis")
                    file_input = gr.File(label="PDF UPLOAD", file_types=[".pdf"], height=400)
                    status = gr.Textbox(label="Status", value="Waiting...", interactive=False)


                # ìš°ì¸¡ ì±„íŒ…ì°½
                with gr.Column(scale=7):
                    chatbot = gr.Chatbot(
                        label="Medical Consultation",
                        height=600,
                        type="tuples",
                        bubble_full_width=False,
                        avatar_images=(None, "https://cdn-icons-png.flaticon.com/512/3774/3774299.png")
                    )
                    with gr.Row():
                        msg = gr.Textbox(scale=9, show_label=False, placeholder="Enter your question...", autofocus=True)
                        btn_send = gr.Button("Send", scale=1, variant="primary")
                    gr.Markdown("*âš ï¸ Disclaimer: For an accurate diagnosis, please consult a physician.*")

            # ì´ë²¤íŠ¸ ë°”ì¸ë”©
            file_input.upload(self._adapter_ingest, inputs=[file_input], outputs=[status])

            msg.submit(self._adapter_ask, inputs=[msg, chatbot], outputs=[msg, chatbot])
            btn_send.click(self._adapter_ask, inputs=[msg, chatbot], outputs=[msg, chatbot])

        print("ğŸš€ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        demo.launch(share=True, debug=True)

# ==========================================
# 6. ë©”ì¸ ì‹¤í–‰ (Main Execution)
# ==========================================
if __name__ == "__main__":
    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = MedicalRAGSystem()

    # 2. UI ì‹¤í–‰
    ui = MedicalWebUI(rag_system)
    ui.launch()