{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T08:00:39.928119Z",
          "iopub.status.busy": "2026-01-19T08:00:39.92791Z",
          "iopub.status.idle": "2026-01-19T08:00:56.040709Z",
          "shell.execute_reply": "2026-01-19T08:00:56.039996Z",
          "shell.execute_reply.started": "2026-01-19T08:00:39.928088Z"
        },
        "id": "aFUbbtN5S9nI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP7G2YusS9nL"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/txgemma-9b-chat\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/txgemma-9b-chat)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ğŸ™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NF1iSVBS9nN"
      },
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ğŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T08:02:28.759566Z",
          "iopub.status.busy": "2026-01-19T08:02:28.758955Z",
          "iopub.status.idle": "2026-01-19T08:02:29.448346Z",
          "shell.execute_reply": "2026-01-19T08:02:29.447588Z",
          "shell.execute_reply.started": "2026-01-19T08:02:28.75953Z"
        },
        "id": "O5VCb0UzS9nN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mbtiGIMS9nO"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/txgemma-9b-chat\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/txgemma-9b-chat\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbut9PpNUrDN"
      },
      "outputs": [],
      "source": [
        "# ê²½ê³  ë©”ì‹œì§€(ERROR: pip's dependency resolver...)ê°€ ë– ë„ ë¬´ì‹œí•˜ê³  ì§„í–‰í•˜ì„¸ìš”.\n",
        "# ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install -q -U langchain langchain-community langchain-huggingface sentence-transformers chromadb accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuJArUADVbMD"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eBSXPnMnpyQ"
      },
      "outputs": [],
      "source": [
        "# rank_bm25(í‚¤ì›Œë“œ ê²€ìƒ‰ìš©) ì„¤ì¹˜\n",
        "!pip install -q rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF7uDPhGU1Nd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "class MedicalSmartRAG:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"google/txgemma-9b-chat\",\n",
        "        embedding_model: str = \"jhgan/ko-sroberta-multitask\"\n",
        "    ):\n",
        "        print(f\"ğŸš€ [{model_id}] RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...\")\n",
        "\n",
        "        # 1. 4-bit ì–‘ìí™” ì„¤ì •\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        # 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\", # GPU ìë™ í• ë‹¹\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # 3. í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "        print(f\"ğŸ“Š ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {embedding_model}\")\n",
        "        self.embed_model = SentenceTransformer(embedding_model)\n",
        "\n",
        "        # 4. ë²¡í„° DB (Chroma) ì´ˆê¸°í™”\n",
        "        self.chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
        "        try:\n",
        "            self.chroma_client.delete_collection(\"medical_kb\")\n",
        "        except:\n",
        "            pass\n",
        "        self.collection = self.chroma_client.create_collection(\"medical_kb\")\n",
        "\n",
        "        print(\"âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "\n",
        "    def ingest_documents(self, documents: list):\n",
        "        \"\"\"\n",
        "        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° DBì— ì €ì¥\n",
        "        documents = [{\"text\": \"...\", \"source\": \"...\"}, ...]\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ“š ë¬¸ì„œ {len(documents)}ê°œ ì²˜ë¦¬ ì¤‘...\")\n",
        "        texts = [doc['text'] for doc in documents]\n",
        "        metadatas = [{\"source\": doc.get('source', 'unknown')} for doc in documents]\n",
        "        ids = [f\"doc_{i}\" for i in range(len(texts))]\n",
        "\n",
        "        # ì„ë² ë”© ìƒì„±\n",
        "        embeddings = self.embed_model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "        # DB ì €ì¥\n",
        "        self.collection.add(\n",
        "            documents=texts,\n",
        "            embeddings=embeddings.tolist(),\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        print(\"ğŸ’¾ ì €ì¥ ì™„ë£Œ.\")\n",
        "\n",
        "    def generate_response(self, query: str):\n",
        "        # 1. ê²€ìƒ‰ (Retrieval)\n",
        "        query_vec = self.embed_model.encode(query).tolist()\n",
        "        results = self.collection.query(query_embeddings=[query_vec], n_results=3)\n",
        "\n",
        "        retrieved_docs = results['documents'][0]\n",
        "        sources = [meta['source'] for meta in results['metadatas'][0]]\n",
        "\n",
        "        # ë¬¸ë§¥ ì¡°í•©\n",
        "        context_str = \"\\n\\n\".join([f\"- ë‚´ìš©: {text}\" for text in retrieved_docs])\n",
        "\n",
        "        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Gemma Chat Template ì¤€ìˆ˜)\n",
        "        # TxGemmaëŠ” ì˜ë£Œ íŠ¹í™”ì´ë¯€ë¡œ ì „ë¬¸ì ì¸ í˜ë¥´ì†Œë‚˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
        "        prompt = f\"\"\"<start_of_turn>user\n",
        "ë‹¹ì‹ ì€ í•œêµ­ì˜ ì˜ë£Œ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ì˜ë£Œ ì§€ì¹¨]ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
        "\n",
        "\n",
        "ì£¼ì˜ì‚¬í•­:\n",
        "1. ìˆ˜ì¹˜ê°€ 'ê¶Œê³ (Target)'í•˜ëŠ” ê²ƒì¸ì§€, 'í”¼í•´ì•¼ í• (Warning)' ê²ƒì¸ì§€ ë¬¸ë§¥ì„ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”.\n",
        "2. 'J-ëª¨ì–‘ ê´€ë ¨ì„±'ì´ë‚˜ 'ì‚¬ë§ë¥  ì¦ê°€'ì™€ ê´€ë ¨ëœ ìˆ˜ì¹˜ëŠ” ëª©í‘œ ìˆ˜ì¹˜ê°€ ì•„ë‹™ë‹ˆë‹¤.\n",
        "3. ì§€ì¹¨ì— ì—†ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.\n",
        "\n",
        "[ì˜ë£Œ ì§€ì¹¨]\n",
        "{context_str}\n",
        "\n",
        "ì§ˆë¬¸: {query}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "        # 3. ë‹µë³€ ìƒì„± (Generation)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,       # ë‹µë³€ ê¸¸ì´\n",
        "                temperature=0.2,          # ì‚¬ì‹¤ ê¸°ë°˜ì´ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •\n",
        "                repetition_penalty=1.1,   # ë°˜ë³µ ë°©ì§€\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        # 4. ê²°ê³¼ ë””ì½”ë”©\n",
        "        answer = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer.strip(),\n",
        "            \"sources\": list(set(sources)) # ì¤‘ë³µ ì œê±°\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCnj-mfeZxvn"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcbIOA3peiDR"
      },
      "outputs": [],
      "source": [
        "#pdf í‘œ ì¸ì‹ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ã…£\n",
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXG_X0ssf07r"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PFglw4vYQ9q"
      },
      "outputs": [],
      "source": [
        "# 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "rag_bot = MedicalSmartRAG()\n",
        "\n",
        "import pdfplumber\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def table_to_markdown(table):\n",
        "    \"\"\"\n",
        "    pdfplumberë¡œ ì¶”ì¶œí•œ í‘œ(List[List[str]])ë¥¼ Markdown í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "    \"\"\"\n",
        "    if not table or len(table) < 2:\n",
        "        return \"\"\n",
        "\n",
        "    # None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "    table = [['' if cell is None else cell for cell in row] for row in table]\n",
        "\n",
        "    try:\n",
        "        # í—¤ë” ìƒì„±\n",
        "        markdown = \"| \" + \" | \".join(map(str, table[0])) + \" |\\n\"\n",
        "        markdown += \"| \" + \" | \".join([\"---\"] * len(table[0])) + \" |\\n\"\n",
        "\n",
        "        # ë°ì´í„° í–‰ ìƒì„±\n",
        "        for row in table[1:]:\n",
        "            markdown += \"| \" + \" | \".join(map(str, row)) + \" |\\n\"\n",
        "        return markdown + \"\\n\"\n",
        "    except Exception as e:\n",
        "        return \"\" # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜\n",
        "\n",
        "def process_pdf_with_tables(file_path):\n",
        "    print(f\"ğŸ“„ PDF ë¡œë”© ë° í‘œ ì²˜ë¦¬ ì¤‘: {file_path}\")\n",
        "    docs = []\n",
        "\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for i, page in enumerate(pdf.pages):\n",
        "            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "            text = page.extract_text() or \"\"\n",
        "\n",
        "            # 2. í‘œ ì¶”ì¶œ ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜\n",
        "            tables = page.extract_tables()\n",
        "            table_text = \"\"\n",
        "            if tables:\n",
        "                for table in tables:\n",
        "                    table_text += table_to_markdown(table)\n",
        "\n",
        "            # 3. í…ìŠ¤íŠ¸ì™€ í‘œ ë‚´ìš©ì„ í•©ì¹¨ (í‘œê°€ í…ìŠ¤íŠ¸ ì•„ë˜ì— ì˜¤ë„ë¡)\n",
        "            # íŒ: í‘œê°€ ì¤‘ìš”í•˜ë‹¤ë©´ table_textë¥¼ textë³´ë‹¤ ì•ì— ë‘˜ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "            combined_content = f\"{text}\\n\\n[í‘œ ë°ì´í„°]\\n{table_text}\"\n",
        "\n",
        "            # 4. LangChain Document ê°ì²´ ìƒì„±\n",
        "            docs.append(Document(\n",
        "                page_content=combined_content,\n",
        "                metadata={\"source\": f\"{file_path} (Page {i + 1})\", \"page\": i + 1}\n",
        "            ))\n",
        "\n",
        "    # 5. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)\n",
        "    # Markdown í˜•ì‹ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ ì¡°ê¸ˆ ë” ë„‰ë„‰í•˜ê²Œ ì¡ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,       # í‘œê°€ í¬í•¨ë˜ë¯€ë¡œ í¬ê¸°ë¥¼ ëŠ˜ë¦¼ (500 -> 800)\n",
        "        chunk_overlap=350,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \"|\", \".\", \" \"] # í‘œì˜ êµ¬ë¶„ì('|')ë¥¼ separatorì— ì¶”ê°€\n",
        "    )\n",
        "\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # 6. ë°ì´í„° í¬ë§· ë³€í™˜ (MedicalSmartRAG í˜¸í™˜ìš©)\n",
        "    processed_docs = []\n",
        "    for doc in splits:\n",
        "        processed_docs.append({\n",
        "            \"text\": doc.page_content,\n",
        "            \"source\": doc.metadata['source']\n",
        "        })\n",
        "\n",
        "    print(f\"âœ… ì´ {len(docs)}í˜ì´ì§€ë¥¼ {len(processed_docs)}ê°œì˜ ì²­í¬(í‘œ í¬í•¨)ë¡œ ë¶„í•  ì™„ë£Œ.\")\n",
        "    return processed_docs\n",
        "\n",
        "# --- ì‹¤í–‰ë¶€ ---\n",
        "\n",
        "# ì—…ë¡œë“œí•œ PDF íŒŒì¼ ê²½ë¡œ\n",
        "pdf_path = \"/content/drive/MyDrive/[ëŒ€í•œì˜í•™íšŒ] ë‹¹ë‡¨ë³‘ ì„ìƒì§„ë£Œì§€ì¹¨.pdf\"\n",
        "\n",
        "# ìƒˆë¡œìš´ í•¨ìˆ˜ë¡œ ì‹¤í–‰\n",
        "real_guidelines = process_pdf_with_tables(pdf_path)\n",
        "\n",
        "# ê¸°ì¡´ RAG ë´‡ì— ì£¼ì…\n",
        "rag_bot.ingest_documents(real_guidelines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAZUvSenbVth"
      },
      "outputs": [],
      "source": [
        "# 4. ì§ˆë¬¸í•˜ê¸°\n",
        "q1 = input(\"ì§ˆë¬¸: \")\n",
        "result = rag_bot.generate_response(q1)\n",
        "print(f\"ë‹µë³€:\\n{result['answer']}\")\n",
        "print(f\"ì¶œì²˜: {result['sources']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
